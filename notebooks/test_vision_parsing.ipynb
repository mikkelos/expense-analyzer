{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.cloud import datastore\n",
    "from google.oauth2 import service_account\n",
    "import io\n",
    "import os # for use with setting env variables\n",
    "import re\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "* Discount line logic\n",
    "* Create an interface where the user can manually give an item its category:\n",
    "    * Remember ID and update the category_id, keep other values as the same\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "developing = True\n",
    "local_run = True\n",
    "key_path = \"/Volumes/GoogleDrive/My Drive/00. My Documents/03. Internt/24. Expense analyzer/config_files/expense-analyzer-260008-0cac2ecd3671.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only used in dev environment\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")\n",
    "datastore_client = datastore.Client(\n",
    "    credentials=credentials\n",
    ")\n",
    "vision_client = vision.ImageAnnotatorClient(\n",
    "    credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERLAPPING_ALLOCATION_THRESHOLD = 0.3\n",
    "\n",
    "# The entity kind in datastore to query to find previous assignments\n",
    "DATASTORE_KIND_CATEGORY_ASSIGNMENT = \"category_item_mapping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_rotate(path):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "\n",
    "    # vision_client = vision.ImageAnnotatorClient()\n",
    "    \"\"\"\n",
    "    blob = storage_client.bucket(bucket_name).get_blob(file_name)\n",
    "    blob_uri = f'gs://{bucket_name}/{file_name}'\n",
    "    blob_source = {'source': {'image_uri': blob_uri}}\n",
    "    # Ignore already-blurred files\n",
    "    if file_name.startswith('blurred-'):\n",
    "        print(f'The image {file_name} is already blurred.')\n",
    "        return\n",
    "\n",
    "    print(f'Analyzing {file_name}.')\n",
    "    result = vision_client.safe_search_detection(blob_source)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" ONLY USED FOR LOCAL FILES \"\"\"\n",
    "    if local_run:\n",
    "        with io.open(path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "\n",
    "        image = vision.types.Image(content=content)\n",
    "    else:\n",
    "        image = {'source': {'image_uri': path}}\n",
    "\n",
    "    response = vision_client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "\n",
    "    if debug:\n",
    "        print('Texts:')\n",
    "        for text in texts:\n",
    "            print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "            vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                        for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "            print('bounds: {}'.format(','.join(vertices)))\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ### PARSER FOR COOP\n",
    "def article_lines_coop(response):\n",
    "    # Locate the relevant range to extract items\n",
    "    start_y_coordinate = -1     # Determine which point to start extract items\n",
    "    end_y_coordinate = -1       # Determine which point to stop extract items\n",
    "\n",
    "    receipt_id_and_datetime = \"\"\n",
    "\n",
    "    for text in response.text_annotations:\n",
    "        if len(text.description) > 100:\n",
    "            # Full text, extract the receipt ID and datetime\n",
    "            full_text = text.description\n",
    "\n",
    "            # Start substring from the search term\n",
    "            search_term = \"Salgskvittering\"\n",
    "            start_index = full_text.find(search_term) + len(search_term)\n",
    "\n",
    "            # End the substring at next newline\n",
    "            end_index = start_index + full_text[start_index:].find(\"\\n\")\n",
    "\n",
    "            # This now contains id, date, time, separated by space\n",
    "            receipt_id_and_datetime = full_text[start_index:end_index].strip()\n",
    "        elif \"Salgskv\" in text.description:\n",
    "            start_y_coordinate = max(vertex.y for vertex in text.bounding_poly.vertices)\n",
    "            if debug:\n",
    "                print(\"Found starting point at {}, after text {}\".format(start_y_coordinate, text.description))\n",
    "        elif \"Totalt\" in text.description:\n",
    "            end_y_coordinate = min(vertex.y for vertex in text.bounding_poly.vertices)\n",
    "            if debug:\n",
    "                print(\"Found ending point at {}, after text {}\".format(end_y_coordinate, text.description))\n",
    "\n",
    "    # Iterate through all lines, extract only those with item y coordinate larger than start and smaller than end\n",
    "    relevant_items = []\n",
    "    for text in response.text_annotations:\n",
    "        if text.bounding_poly.vertices[0].y > start_y_coordinate and text.bounding_poly.vertices[0].y < end_y_coordinate:\n",
    "            # print(\"Found an item line!: {}\".format(text.description))\n",
    "            relevant_items.append(text)\n",
    "\n",
    "    return relevant_items, receipt_id_and_datetime\n",
    "\n",
    "\n",
    "# Key = line_number, value = item\n",
    "# Idea: For each bounding box, calculate the mid y coordinate. If this coordinate is inside the bounding box of\n",
    "# another, then these are on the same line.\n",
    "def allocate_lines_coop(items):\n",
    "    \"\"\"\n",
    "    :param items: input is a list of relevant text boxes from Google vision, containing the text found and bounding polygon\n",
    "    :return: returns a dictionary, with all items allocated to a line_id containing all elements on that same line, sorted by their x-coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    receipt_lines = {}\n",
    "    # Key is the line number\n",
    "    # Each value has the format [item]\n",
    "\n",
    "    # Loop over all found text boxes\n",
    "    for item in items:\n",
    "        y_first_coor = item.bounding_poly.vertices[0].y\n",
    "        y_fourth_coor = item.bounding_poly.vertices[3].y\n",
    "        y_mean_coor = (y_first_coor + y_fourth_coor)/2\n",
    "        height = y_fourth_coor - y_first_coor\n",
    "\n",
    "        overlap_up = -1\n",
    "        overlap_down = -1\n",
    "\n",
    "        if len(receipt_lines) == 0:\n",
    "            receipt_lines[0] = []\n",
    "            receipt_lines[0].append(item)\n",
    "\n",
    "        else:\n",
    "            inserted = False\n",
    "\n",
    "            # Loop through all allocated/identified lines\n",
    "            for line in receipt_lines:\n",
    "                # See if item belongs to an existing line\n",
    "                # Compare against y coordinates of first item on line\n",
    "                first_line_item = receipt_lines[line][0]\n",
    "                first_line_item_y1 = first_line_item.bounding_poly.vertices[0].y\n",
    "                first_line_item_y4 = first_line_item.bounding_poly.vertices[3].y\n",
    "\n",
    "                # if mean coordinate is within min and max of line, add it to the line\n",
    "                if first_line_item_y1 <= y_mean_coor <= first_line_item_y4:\n",
    "                    receipt_lines[line].append(item)\n",
    "                    inserted = True\n",
    "                    break\n",
    "\n",
    "                # These are used to calculate overlap between lines\n",
    "                last_line_item = receipt_lines[line][-1]\n",
    "                last_line_item_y1 = last_line_item.bounding_poly.vertices[0].y\n",
    "                last_line_item_y4 = last_line_item.bounding_poly.vertices[3].y\n",
    "\n",
    "                # Calculate a match% against each other line, to see if picture is slightly squished\n",
    "                # Calculate against the item to the far right in the current line\n",
    "                if last_line_item_y1 <= y_first_coor <= last_line_item_y4:\n",
    "                    # Some overlap detected. item is below the line in comparison\n",
    "                    overlap_down = float(last_line_item_y4 - y_first_coor) / height\n",
    "                    if debug:\n",
    "                        print(\"Found {}% overlap under between text {} and {} on line {}\".format(overlap_down, item.description, first_line_item.description, line))\n",
    "\n",
    "                if last_line_item_y1 <= y_fourth_coor <= last_line_item_y4:\n",
    "                    # Some overlap detected. item is above the line in comparison\n",
    "                    overlap_up = float(y_fourth_coor - last_line_item_y1) / height\n",
    "                    if debug:\n",
    "                        print(\"Found {}% overlap over between text {} and {} (first item) on line {}\".format(overlap_up, item.description, first_line_item.description, line))\n",
    "\n",
    "                # If any of the matches are above X%, allocate it to that line\n",
    "                if overlap_down > OVERLAPPING_ALLOCATION_THRESHOLD or overlap_up > OVERLAPPING_ALLOCATION_THRESHOLD:\n",
    "                    receipt_lines[line].append(item)\n",
    "                    inserted = True\n",
    "                    break\n",
    "\n",
    "            # No match found against previous lines. Create a new line\n",
    "            if not inserted:\n",
    "                new_line_num = len(receipt_lines)\n",
    "                receipt_lines[new_line_num] = []\n",
    "                receipt_lines[new_line_num].append(item)\n",
    "\n",
    "    # Sort each line by the x coordinates\n",
    "    for line in receipt_lines:\n",
    "        receipt_lines[line].sort(key=lambda item: item.bounding_poly.vertices[0].x)\n",
    "\n",
    "    return receipt_lines\n",
    "\n",
    "\n",
    "def lines_to_text(receipt_lines):\n",
    "    \"\"\"\n",
    "    :param receipt_lines: takes a dictionary as input, where the key is a\n",
    "            line_id and the value are objects containing the\n",
    "            element text and bounding polygon\n",
    "    :return: A list of text strings concatenated for each line, instead of\n",
    "             google vision objects\n",
    "    \"\"\"\n",
    "    receipt_text = []\n",
    "    for line in receipt_lines:\n",
    "        text = \"\"\n",
    "        for item in receipt_lines[line]:\n",
    "            text += \" \" + item.description\n",
    "        receipt_text.append(text.lower().strip())\n",
    "    return receipt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preparation(receipt_text):\n",
    "    \"\"\"\n",
    "    :param receipt_text: list of text strings containing article text and price\n",
    "    :return: a list of dict line items, with keys:\n",
    "            item_name: The full item name\n",
    "            item_count: The number of items\n",
    "            type: \n",
    "            price_gross: The total price before discount\n",
    "            price_net: The total price after discount\n",
    "            unit_price_net: The unit price after dicount\n",
    "            discount_amt: The total discount\n",
    "            discount_type: Percentage, fixed, mix and match, etc    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Regex:\n",
    "        \"\\d+\" matches one or more digits\n",
    "        \".\" followed by any charcter\n",
    "        \"\\d+\" one or more digits\n",
    "        \"$\" at the end of the line\n",
    "    \"\"\"\n",
    "    price_pattern = \"(\\d+.\\d+)$\"\n",
    "    discont_pattern = \"\"\n",
    "    articles_querified = []\n",
    "\n",
    "    number_of_lines = 0\n",
    "    \n",
    "    curr_item_line = {}\n",
    "    \n",
    "    for article in receipt_text:\n",
    "        if debug:\n",
    "            print(\"------------\")\n",
    "            print(article)\n",
    "\n",
    "        if \"rabatt\" in article:\n",
    "            # TODO: x = re.spltt(discount_pattern, article)\n",
    "            # articles.append([\"discount\", x[0], x[1]])\n",
    "            \n",
    "            discount_line = article.split(\" \")\n",
    "            # Will have the structure: ['rabatt:', 'nok', '13.16', '(40%', 'av', '32.90)']\n",
    "            \n",
    "            if len(discount_line) >= 3:\n",
    "                discount_amt = discount_line[2]\n",
    "                \n",
    "                # A line starting with \"Rabatt\" belongs to the last item identified\n",
    "                curr_item_line[\"discount_amt\"] = discount_amt\n",
    "                curr_item_line[\"discount_type\"] = \"percentage\"\n",
    "            else:\n",
    "                print(\"Found an unknown discount format\".format(article))\n",
    "            \n",
    "            if debug:\n",
    "                print(\"Found a discount line\")\n",
    "                print(article.split(\" \"))\n",
    "\n",
    "        elif \"antall\" in article:\n",
    "            # Do something TODO\n",
    "            \n",
    "            antall_line = article.split(\" \")\n",
    "            # Will have the structure: ['antall:', '2', 'stk', '1.60', 'kr/stk']\n",
    "            \n",
    "            if len(discount_line) >= 4:\n",
    "                item_count = antall_line[1]\n",
    "                unit_price_net = antall_line[3]\n",
    "                \n",
    "                # A line starting with \"antall\" belongs to the last item identified\n",
    "                curr_item_line[\"item_count\"] = item_count\n",
    "                curr_item_line[\"unit_price_net\"] = unit_price_net\n",
    "                         \n",
    "            if debug:\n",
    "                print(\"Found a antall line\")\n",
    "                print(article.split(\" \"))\n",
    "\n",
    "        elif \"artikler\" not in article:\n",
    "            curr_item_line = {}\n",
    "            x = re.split(price_pattern, article, 2)\n",
    "\n",
    "            # Element 3, index 2, is always empty string\n",
    "            if len(x) == 3:\n",
    "                # actual article\n",
    "                item = x[0].strip()\n",
    "                price = x[1].strip()\n",
    "                \n",
    "                curr_item_line[\"item_name\"] = item\n",
    "                curr_item_line[\"price_net\"] = price\n",
    "                curr_item_line[\"price_gross\"] = price\n",
    "                curr_item_line[\"unit_price_net\"] = price\n",
    "                curr_item_line[\"item_count\"] = 1\n",
    "                \n",
    "                articles_querified.append(curr_item_line)\n",
    "                if debug:\n",
    "                    print(\"len was 3\")\n",
    "                    print(\"Appending item '{}' with price '{}'. Full split is '{}'\".format(item, price, x))\n",
    "            else:\n",
    "                # Typically weight times price per kg.\n",
    "                if developing:\n",
    "                    print(\"Unparsable line: {}\".format(article))\n",
    "        else:\n",
    "            if developing:\n",
    "                print(\"Unparsable line 2: {}\".format(article))\n",
    "                \n",
    "    # Finally, iterate through the set of lines and update fields that are calculated by discounts\n",
    "    for article in articles_querified:\n",
    "        if \"discount_amt\" in article and \"price_net\" in article:\n",
    "            \n",
    "            discount = article.get(\"discount_amt\").strip()\n",
    "            price_net = article.get(\"price_net\").strip()\n",
    "            \n",
    "            # Try convert to floats\n",
    "            try:\n",
    "                discount = float(discount)\n",
    "                price_net = float(price_net)\n",
    "                float_success = True\n",
    "            except ValueError:\n",
    "                float_success = False\n",
    "                print(\"Unable to parse either {} or {}\".format(discount, price_net))\n",
    "            \n",
    "            # Add discount to calculate gross price. Check parsability\n",
    "            if float_success:\n",
    "                article[\"price_gross\"] = str(round((discount + price_net)*100)/100)\n",
    "    \n",
    "    return articles_querified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_item_category(item_name):\n",
    "    \"\"\" Search through similar items and reuse their category\n",
    "        Give it a category of 0 if it not seen before\n",
    "    \"\"\"\n",
    "\n",
    "    query = datastore_client.query(kind=DATASTORE_KIND_CATEGORY_ASSIGNMENT)\n",
    "    # query.add_filter(\"item_name\", \"=\", item_name)\n",
    "\n",
    "    # Create a filter on the key\n",
    "    first_key = datastore_client.key(DATASTORE_KIND_CATEGORY_ASSIGNMENT, item_name)\n",
    "    query.key_filter(first_key, '=')\n",
    "\n",
    "    # Fetch only one result\n",
    "    q_result = query.fetch(limit=1)\n",
    "\n",
    "    category_id = 0\n",
    "    for res in q_result:\n",
    "        category_id = res[\"cat_id\"]\n",
    "\n",
    "    return category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToDatastore(articles_querified, added_by, trans_datetime, receipt_id):\n",
    "    \"\"\" Writes the arcitles to the datastore. On the way, look up the category\n",
    "        mapping if this item has been categorized before. If not found, it will\n",
    "        be created with an id of -1, so that it can be updated later\n",
    "    :param articles_querified: list of text strings containing article text and price\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "\n",
    "    kind = 'transaction'  # The kind for the new entity\n",
    "    now = datetime.now()  # Registered datetime\n",
    "    \n",
    "    task_list = []        # Used for bulk upload to datastore\n",
    "    \n",
    "    # Loop over all articles and insert one by one\n",
    "    for article in articles_querified:\n",
    "        item = article.get(\"item_name\", 0)\n",
    "        category_id = fetch_item_category(item)\n",
    "        if debug:\n",
    "            print(\"Assignning category {} to item {}\".format(category_id, item))\n",
    "\n",
    "        # Only happens if we have not seen this item before. Then we add it to\n",
    "        # unmapped items with an id of -1.\n",
    "        if category_id == 0:\n",
    "            cat_task_key = datastore_client.key(DATASTORE_KIND_CATEGORY_ASSIGNMENT, item)\n",
    "            cat_task = datastore.Entity(key=cat_task_key)\n",
    "            cat_task[\"cat_id\"] = -1\n",
    "            datastore_client.put(cat_task)\n",
    "            print('Saved {}: {}'.format(cat_task.key.name, cat_task['cat_id']))\n",
    "\n",
    "        # The Cloud Datastore key for the new entity. Creating with partial key\n",
    "        task_key = datastore_client.key(kind)\n",
    "\n",
    "        # Prepares the new entity\n",
    "        task = datastore.Entity(key=task_key)\n",
    "        task['added_by'] = added_by\n",
    "        task['cat_id'] = category_id\n",
    "        task['discount_amt'] = article.get(\"discount_amt\", 0)\n",
    "        task['discount_type'] = article.get(\"discount_type\", 0)\n",
    "        task['item_id'] = 0  # Missing\n",
    "        task['item_name'] = item\n",
    "        task['price_gross'] = article.get(\"price_gross\", 0)\n",
    "        task['price_net'] = article.get(\"price_net\", 0)\n",
    "        task[\"item_count\"] = article.get(\"item_count\",0)\n",
    "        task[\"unit_price_net\"] = article.get(\"unit_price_net\", 0)\n",
    "        task['registered_datetime'] = now\n",
    "        task['trans_date'] = trans_datetime\n",
    "        task['receipt_id'] = receipt_id\n",
    "\n",
    "        if debug:\n",
    "            print(\"Writting to datastore:\", task)\n",
    "\n",
    "        # Saves a single entity:\n",
    "        # datastore_client.put(task)\n",
    "        task_list.append(task)\n",
    "        print('Added {}: {}'.format(task.key.name, task['item_name']))\n",
    "        \n",
    "        # End for loop\n",
    "        \n",
    "    # Saves multiple entities at once: \n",
    "    datastore_client.put_multi(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\NO007454\\Documents\\03. Internt\\24. Expense analyzer\\test_images\\IMG_1010.JPEG\"\n",
    "path = r\"C:\\Users\\NO007454\\Documents\\03. Internt\\24. Expense analyzer\\test_images\\IMG_1012.JPEG\"\n",
    "path = \"/Volumes/GoogleDrive/My Drive/00. My Documents/03. Internt/24. Expense analyzer/test_images/IMG_1012.JPEG\"\n",
    "path = \"/Volumes/GoogleDrive/My Drive/00. My Documents/03. Internt/24. Expense analyzer/test_images/IMG_20200211.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMG_1010.JPEG',\n",
       " 'IMG_1062.JPEG',\n",
       " 'IMG_1044.JPEG',\n",
       " 'IMG_1012.JPEG',\n",
       " 'IMG_1011.JPEG',\n",
       " 'IMG_0156.HEIC',\n",
       " 'IMG_20200211.png']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "response = detect_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_lines, receipt_id_and_datetime = article_lines_coop(response)\n",
    "receipt_lines = allocate_lines_coop(relevant_lines)\n",
    "actual_lines = lines_to_text(receipt_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unparsable line 2: artikler) 523.02\n"
     ]
    }
   ],
   "source": [
    "articles_querified = query_preparation(actual_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the receipt id and datetime before saving to datastore\n",
    "receipt_id = receipt_id_and_datetime.split(\" \")[0].strip()\n",
    "receipt_date = receipt_id_and_datetime.split(\" \")[1].strip()\n",
    "datetime_object = datetime.strptime(receipt_date, '%m.%d.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added None: b.d. oyster sauce\n",
      "Added None: buer potetlomper\n",
      "Added None: bærepose mega\n",
      "Added None: coca cola uts 1.5l\n",
      "Added None: pant\n",
      "Added None: coop fiber frøbrød\n",
      "Added None: fullkornris 500g\n",
      "Added None: gilde karbonader\n",
      "Added None: gilde krydderskinke\n",
      "Added None: grand. nybakt skinke\n",
      "Added None: mills postei 185g\n",
      "Added None: naan bread garlic\n",
      "Added None: penne rigate fullkor\n",
      "Added None: philadelphia 200g\n",
      "Added None: plantego paprika\n",
      "Added None: saritas tandoor saus\n",
      "Added None: tikka masala saritas\n",
      "Added None: *zend.fresh+whi.75ml\n"
     ]
    }
   ],
   "source": [
    "writeToDatastore(articles_querified, \"testuser\", datetime_object, receipt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that have changed: query_preparation\n",
    "# Need to change: writeToDatastore, populate fields based on dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_expense_analyzer",
   "language": "python",
   "name": "env_expense_analyzer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
